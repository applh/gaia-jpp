{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraper for tasks\n",
    "\n",
    "* extract infos from web pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def scrap_page (url, index, site):\n",
    "    # if url is not set, return\n",
    "    if not url:\n",
    "        return\n",
    "    try:\n",
    "        from playwright.async_api import async_playwright\n",
    "        playwright = await async_playwright().start()\n",
    "        browser = await playwright.chromium.launch()\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        print(f\"Scraping {url} ...\")\n",
    "        await page.goto(url)\n",
    "\n",
    "        # wait for page to load\n",
    "        await page.wait_for_load_state('networkidle')\n",
    "         \n",
    "        # scrap selector\n",
    "        if 'selector' in site:\n",
    "            selector = site['selector']\n",
    "            # get all elements\n",
    "            elements = await page.query_selector_all(selector)\n",
    "\n",
    "            # print number of results\n",
    "            print(f\"Found {len(elements)} results\")\n",
    "\n",
    "            # print all elements text\n",
    "            for element in elements:\n",
    "                found = {}\n",
    "                # select title in element\n",
    "                select_title = site['select_title'] if 'select_title' in site else None\n",
    "                if select_title:\n",
    "                    title = await element.query_selector(select_title)\n",
    "                    title_text = await title.inner_text()\n",
    "                    found['title'] = title_text\n",
    "                    # print(title_text)\n",
    "                # select link in element\n",
    "                select_link = site['select_link'] if 'select_link' in site else None\n",
    "                if select_link:\n",
    "                    link = await element.query_selector(select_link)\n",
    "                    link_href = await link.get_attribute('href')\n",
    "                    found['link'] = link_href\n",
    "\n",
    "                    # if link is relative, add base url\n",
    "                    if link_href.startswith('/'):\n",
    "                        base_url = site['base_url'] if 'base_url' in site else None\n",
    "                        if base_url:\n",
    "                            # remove query string from link_href\n",
    "                            if '?' in link_href:\n",
    "                                link_href = link_href.split('?')[0]\n",
    "                            link_href = f\"{base_url}{link_href}\"\n",
    "                            found['link_full'] = link_href\n",
    "\n",
    "                            # compute md5 hash of link_href\n",
    "                            import hashlib\n",
    "                            found['link_hash'] = hashlib.md5(link_href.encode()).hexdigest()\n",
    "\n",
    "                    # print(link_href)\n",
    "                # select time in element\n",
    "                select_time = site['select_time'] if 'select_time' in site else None\n",
    "                if select_time:\n",
    "                    time = await element.query_selector(select_time)\n",
    "                    time_text = await time.get_attribute('datetime')\n",
    "                    found['time'] = time_text\n",
    "                    # print(time_text)\n",
    "                # add found to site\n",
    "                print(found)\n",
    "                if 'results' not in site:\n",
    "                    site['results'] = []\n",
    "\n",
    "                site['results'].append(found)          \n",
    "\n",
    "\n",
    "        # save screenshot\n",
    "        # get name of site\n",
    "        name = site['name'] if 'name' in site else 'unknown'\n",
    "        output = site['output'] if 'output' in site else '.'\n",
    "        target_file = f\"{output}/screenshot-{name}-{index}.png\"\n",
    "        await page.screenshot(path=target_file, full_page=True)\n",
    "        await browser.close()\n",
    "        await playwright.stop()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    print(site)\n",
    "    \n",
    "    # optional: display screenshot\n",
    "    show_image = site['show_image'] if 'show_image' in site else False\n",
    "    if show_image:\n",
    "        # if file exists, display it\n",
    "        import os.path\n",
    "        if os.path.isfile(target_file):\n",
    "            from IPython.display import Image\n",
    "            img = Image(filename=target_file)\n",
    "            display(img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_scraps (site):\n",
    "    db_path = site['sqlite'] if 'sqlite' in site else '../my-data/db-scraps.sqlite'\n",
    "\n",
    "    # check if the file exists\n",
    "    import os.path\n",
    "    # WARNING: the path is relative to the current directory\n",
    "    if os.path.isfile(db_path):\n",
    "        print(f\"The DB file exists: {db_path}\")\n",
    "        return\n",
    "    \n",
    "    # create a sqlite database\n",
    "    import sqlite3\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # create a table with columns id, title, url, date, source is not exists\n",
    "    c.execute('CREATE TABLE IF NOT EXISTS news (id INTEGER PRIMARY KEY, md5 TEXT, tag TEXT, title TEXT, url TEXT, date TEXT)')\n",
    "    c.execute('CREATE UNIQUE INDEX IF NOT EXISTS idx_md5 ON news (md5)')\n",
    "\n",
    "    # commit the changes\n",
    "    conn.commit()\n",
    "\n",
    "    # close the connection\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_db_scraps (site):\n",
    "    create_db_scraps(site)\n",
    "    \n",
    "    results = site['results'] if 'results' in site else []\n",
    "    db_path = site['sqlite'] if 'sqlite' in site else '../my-data/db-scraps.sqlite'\n",
    "\n",
    "    # insert into the database table news\n",
    "    import sqlite3\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    # for each result\n",
    "    nb_dup = 0\n",
    "    for result in results:\n",
    "        # check if the md5 hash is already in the database\n",
    "        md5 = result['link_hash'] if 'link_hash' in result else None\n",
    "        if md5:\n",
    "            c.execute('SELECT * FROM news WHERE md5=?', (md5,))\n",
    "            row = c.fetchone()\n",
    "            if row:\n",
    "                print(f\"Already in database: {row}\")\n",
    "                nb_dup += 1\n",
    "                continue\n",
    "        # insert into the database\n",
    "        c.execute('INSERT INTO news (md5, tag, title, url, date) VALUES (?, ?, ?, ?, ?)', (md5, site['name'], result['title'], result['link_full'], result['time']))\n",
    "\n",
    "    # commit the changes\n",
    "    conn.commit()\n",
    "    # close the connection\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"Inserted {len(results) - nb_dup} new results in the database\")\n",
    "    print(f\"Skipped {nb_dup} duplicates\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrap_site (site):\n",
    "    url = site['url'] if 'url' in site else None\n",
    "    # if url is not set, return\n",
    "    if not url:\n",
    "        return\n",
    "    \n",
    "    page_start = site['page_start'] if 'page_start' in site else 1\n",
    "    page_end = site['page_end'] if 'page_end' in site else 1\n",
    "\n",
    "    for page_index in range(page_start, page_end + 1):\n",
    "        print(f\"Page {page_index}\")\n",
    "        await scrap_page(f\"{url}{page_index}\", page_index, site)\n",
    "        # maybe stop if max results is reached\n",
    "\n",
    "    # save results\n",
    "    # path_results = f\"{path_srap}/results-YYMMDD-HHIISS.json\"\n",
    "    import datetime\n",
    "    now = datetime.datetime.now()\n",
    "\n",
    "    path_scrap = site['output'] if 'output' in site else '.'\n",
    "    path_results = f\"{path_scrap}/results-{now.strftime('%Y%m%d-%H%M%S')}.json\"\n",
    "    import json\n",
    "    with open(path_results, 'w') as outfile:\n",
    "        json.dump(site, outfile, indent=4)\n",
    "\n",
    "    update_db_scraps (site)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json config my-config.json\n",
    "config = {}\n",
    "import json\n",
    "with open('my-config.json') as json_file:\n",
    "    config = json.load(json_file)\n",
    "    # print(config)\n",
    "    scraps = config['scraps'] if 'scraps' in config else []\n",
    "    # print(scraps)\n",
    "    # select random scrap\n",
    "    import random\n",
    "    site = random.choice(scraps)\n",
    "    print(site)\n",
    "    await scrap_site(site)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
